{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "This notebook implements the fourth exercise from Andrew Ngâ€™s Machine Learning Course on Coursera.\n",
    "\n",
    "### Problem Description\n",
    "\n",
    "Given a dataset \"ex4data1.mat\", consisting of 5000 training examples of handwritten digits. Each training example is a 20 * 20 pixel grayscale image of the digit. Each pixel is represented by a floating number corresponding to intensity at the location. Implement a Neural Network (feedforward and backpropagation algorithm) for predicting the handwritten digits in the set.\n",
    "\n",
    "Feature set X = 5000 * 400 matrix (each row is training example for handwritten digit image) \n",
    "Target y = 5000 dimension vector with labels for each training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('ex4data1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ...,\n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature, target separation\n",
    "X = data.get('X')\n",
    "y = data.get('y')\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network Model\n",
    "\n",
    "Assume that the neural network model has 3 layers. Layer-1 is input layer consisting of 400 units (size of X), layer-2 is the hidden layer consisting of 25 units and layer-3 is the output layer with 10 units (number of classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 10 # we need to classify 10 labels\n",
    "hidden_units = 25 \n",
    "input_layer = 400\n",
    "regParam = 1 # lambda for regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using 10 units in output layer, we would need an encoder for each class. Let's use the One-hot encoder from Scikit Learn to generate a vector for each class. Out of 10 classes, One-hot encoding turns a class label i into a vector of length 10 where the index i = 1 while the rest are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_vec = encoder.fit_transform(y)\n",
    "y_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets generate random weights for this model. We could use np.random package to generate theta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_vec = (np.random.random(size = hidden_units * (input_layer + 1) + num_labels * (hidden_units + 1)) - 0.5)* 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Back propagation Algorithm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    computes the logistic function\n",
    "    \"\"\"\n",
    "    return (1.0 / (1 + np.exp(-z)))\n",
    "\n",
    "def feedforward(X,theta1, theta2):\n",
    "    \"\"\"\n",
    "    generates hypothesis of neural network model\n",
    "    \"\"\"\n",
    "    # Make sure X, theta1, theta2 are all matrices\n",
    "    m = X.shape[0] # 5000\n",
    "    a1 = np.insert(X, 0, values = np.ones(m), axis = 1) # add bias\n",
    "    z2 = a1 * theta1.T\n",
    "    a2 = np.insert(sigmoid(z2), 0, values = np.ones(m), axis = 1) # add bias\n",
    "    z3 = a2 * theta2.T\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    return [a1,z2,a2,z3,a3] # hypothesis\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    computes the g'(z) needed in backpropagation algorithm\n",
    "    \"\"\"\n",
    "    # irrespective of z (vector, scalar, matrix), compute the sigmoid gradient of z\n",
    "    return (np.multiply(sigmoid(z), (1 - sigmoid(z))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(theta_vec, num_labels, hidden_units, input_layer, X, y, regParam):\n",
    "    \"\"\"\n",
    "     computes the cost, gradient for the given theta\n",
    "    \"\"\"\n",
    "    #theta1 = np.matrix(theta_vec[0 : hidden_units * (input_layer + 1)].reshape(hidden_units, input_layer + 1)) # 25 * 401\n",
    "    #theta2 = np.matrix(theta_vec[hidden_units * (input_layer + 1), :].reshape(num_labels, hidden_units + 1)) # 10 * 26\n",
    "    \n",
    "    theta1 = np.matrix(theta_vec[0: 10025].reshape(25, 401))\n",
    "    theta2 = np.matrix(theta_vec[10025:].reshape(10,26))\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    feed_list = feedforward(X, theta1, theta2)\n",
    "    a1 = feed_list[0]\n",
    "    z2 = feed_list[1]\n",
    "    a2 = feed_list[2]\n",
    "    hyp = feed_list[4]\n",
    "    \n",
    "    # Compute cost\n",
    "    J = 0 #initialize\n",
    "    \n",
    "    for i in range(m):\n",
    "        term1 = np.multiply(-y[i,:], np.log(hyp[i,:]))\n",
    "        term2 = np.multiply((1 - y[i,:]), np.log(1 - hyp[i,:]))\n",
    "        J += np.sum(term1 - term2)\n",
    "    \n",
    "    # adding regularization term\n",
    "    regTerm = np.sum(np.power(theta1[:,1:],2)) + np.sum(np.power(theta2[:,1:],2))\n",
    "    J = J/m + (regParam * regTerm / (2.0 * m))\n",
    "    \n",
    "    # back propagation algorithm \n",
    "    delta1 = np.zeros(theta1.shape) # 25 * 401\n",
    "    delta2 = np.zeros(theta2.shape) # 10 * 26\n",
    "    \n",
    "    for t in range(m):\n",
    "        a1t = a1[t,:] # 1, 401\n",
    "        z2t = z2[t,:] # 1, 25\n",
    "        a2t = a2[t,:] # 1, 26\n",
    "        a3t = hyp[t,:] # 1, 10\n",
    "        yt = y[t,:] # 1, 10\n",
    "        \n",
    "        del3 = a3t - yt # 1, 10\n",
    "        z2t = np.insert(z2t, 0, values=np.ones(1)) # 1, 26\n",
    "        del2 = np.multiply((theta2.T * del3.T).T, sigmoidGradient(z2t)) # 1, 26 \n",
    "        delta1 = delta1 + (del2[:,1:]).T * a1t # 25, 401\n",
    "        delta2 = delta2 + del3.T * a2t # 10, 26\n",
    "        \n",
    "    delta1 = delta1 / m\n",
    "    delta2 = delta2 / m\n",
    "    \n",
    "    # add the gradient regularization term\n",
    "    delta1[:,1:] = delta1[:,1:] + (theta1[:,1:] * regParam) / m\n",
    "    delta2[:,1:] = delta2[:,1:] + (theta2[:,1:] * regParam) / m\n",
    "       \n",
    "    \n",
    "    # unravel the gradient matrices into a single array\n",
    "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))\n",
    "        \n",
    "    return [J, grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Neural Network\n",
    "\n",
    "Lets train the neural network with some random weights \"theta_vec\" and obtain the cost, gradient. This is fed to the advanced optimization algorithm to minimize the cost for chosen theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "J, grad = backpropagation(theta_vec, 400, 25, 10, X, y_vec, regParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.764538576035576, (10285,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J, grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets minimize the cost using the optimization function\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin = minimize(fun=backpropagation, x0=theta_vec, args=(num_labels, hidden_units, input_layer, X, y_vec, regParam), \n",
    "                method='TNC', jac=True, options={'maxiter': 250})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.3327673894451927\n",
       "     jac: array([ 4.27510455e-04,  7.85562093e-07, -3.03278165e-07, ...,\n",
       "       -6.83757147e-05, -7.55343000e-05,  1.02712453e-05])\n",
       " message: 'Max. number of function evaluations reached'\n",
       "    nfev: 250\n",
       "     nit: 21\n",
       "  status: 3\n",
       " success: False\n",
       "       x: array([-1.33437674e+00,  3.92781047e-03, -1.51639083e-03, ...,\n",
       "        1.46193609e+00, -8.86818461e-01, -2.40457629e+00])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Evaluation\n",
    "\n",
    "Using the weights obtained from optimized function, let's predict the value for a randomly selected input and evaluate the accuracy of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1 = np.matrix(np.reshape(fmin.x[0 : hidden_units * (input_layer + 1)], (25, 401)))\n",
    "theta2 = np.matrix(np.reshape(fmin.x[hidden_units * (input_layer + 1) : ], (10, 26)))\n",
    "\n",
    "feed_list = feedforward(X, theta1, theta2)\n",
    "prediction = np.array(np.argmax(feed_list[4], axis = 1) + 1) # feed_list[4] = hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10],\n",
       "       [10],\n",
       "       [10],\n",
       "       ...,\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 9]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 99.33999999999999 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(prediction, y)]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print ('accuracy = {} %'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thats it!! We have succesfully implemented a feed-forward neural network with backpropagation and used it to classify images of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
