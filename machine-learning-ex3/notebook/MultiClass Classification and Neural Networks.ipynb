{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification and Neural Networks\n",
    "\n",
    "This notebook implements the third exercise from Andrew Ngâ€™s Machine Learning Course on Coursera. In this exercise, a one-vs-all logistic regression and neural networks will be implemented to recognize hand-written digits (from 0 to 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Classification\n",
    "\n",
    "Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes) on mail envelopes to recognizing amounts written on bank checks. In this part, the previous exercise of logistic regression will be extended and applied to one-vs-all classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011', '__version__': '1.0', '__globals__': [], 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), 'y': array([[10],\n",
      "       [10],\n",
      "       [10],\n",
      "       ...,\n",
      "       [ 9],\n",
      "       [ 9],\n",
      "       [ 9]], dtype=uint8)}\n"
     ]
    }
   ],
   "source": [
    "data_dict = sio.loadmat('Data/ex3data1.mat')\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature dimensions (5000, 400) and target dimension (5000, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nFeature dimensions {} and target dimension {}\\n'.format(data_dict['X'].shape, data_dict['y'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to classify 10 digits with 5000 training sets, lets approach the problem with logistic regularization technique to avoid over-fitting. Our strategy involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. This algorithm is called One-Vs-Rest or One-against-All."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Gradient and Cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    This function returns hypothesis\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def cost(theta, X, y, regParam):\n",
    "    \"\"\"\n",
    "    This function returns the cost of using theta\n",
    "    \"\"\"\n",
    "    theta = np.matrix(theta)\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    term1 = np.multiply(-y, np.log(sigmoid(X * theta.T)))\n",
    "    term2 = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))\n",
    "    reg = (regParam / 2 * len(X)) * np.sum(np.power(theta[:,1:theta.shape[1]], 2))\n",
    "    return np.sum(term1 - term2) / (len(X)) + reg\n",
    "\n",
    "def gradient(theta, X, y, regParam):\n",
    "    \"\"\"\n",
    "    This function returns the gradient\n",
    "    \"\"\"\n",
    "    theta = np.matrix(theta)\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    error = sigmoid(X * theta.T) - y\n",
    "    \n",
    "    grad = ((X.T * error) / len(X)).T + ((regParam / len(X)) * theta)\n",
    "    \n",
    "    grad[0, 0] = np.sum(np.multiply(error, X[:,0])) / len(X) # theta 0 isn't regularized\n",
    "    \n",
    "    return np.array(grad).ravel()\n",
    "\n",
    "def oneVsAll(X, y, num_labels, regParam):\n",
    "    \"\"\"\n",
    "    This function is the implementation of single class classifier.\n",
    "    Looping over the 10 class will give us the theta estimates\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "        \n",
    "    all_theta = np.zeros((num_labels, n + 1)) # for all 10 classes\n",
    "    \n",
    "    X = np.insert(X, 0, values=np.ones(m), axis=1) # add bias to X (5000, 401)\n",
    "    \n",
    "    for i in range(1, num_labels + 1): # target is labelled from 1..10\n",
    "        theta = np.zeros(n + 1) # theta for each class (401,)\n",
    "        yi = np.array([1 if label == i else 0 for label in y]) \n",
    "        yi = yi.reshape(m,1)\n",
    "        \n",
    "        # minimize the objective function\n",
    "        result = opt.fmin_tnc(func = cost, x0 = theta, fprime = gradient, args= (X, yi, regParam))\n",
    "        \n",
    "        #fmin = minimize(fun=cost, x0=theta, args=(X, yi, regParam), method='TNC', jac=gradient)\n",
    "        all_theta[i-1,:] = result[0]\n",
    "    \n",
    "    return all_theta #(10, 401)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels, regParam = 10, 1\n",
    "\n",
    "\"\"\"\n",
    "X: features will be of dimension (5000, 401) ~ extra bias column\n",
    "y: target will be of dimension (5000, 1)\n",
    "\n",
    "all_theta = theta values for entire 10 class (10, 401)\n",
    "theta = theta for single class (401,)\n",
    "\n",
    "\"\"\"\n",
    "all_theta = oneVsAll(data_dict['X'], data_dict['y'], num_labels, regParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta at which the cost is minimum: [[-3.70247924e-05  0.00000000e+00  0.00000000e+00 ... -2.24803603e-10\n",
      "   2.31962906e-11  0.00000000e+00]\n",
      " [-8.96250749e-05  0.00000000e+00  0.00000000e+00 ...  7.26120890e-09\n",
      "  -6.19965354e-10  0.00000000e+00]\n",
      " [-8.39553305e-05  0.00000000e+00  0.00000000e+00 ... -7.61695535e-10\n",
      "   4.64917608e-11  0.00000000e+00]\n",
      " ...\n",
      " [-7.00832398e-05  0.00000000e+00  0.00000000e+00 ... -6.92008998e-10\n",
      "   4.29241471e-11  0.00000000e+00]\n",
      " [-7.65187918e-05  0.00000000e+00  0.00000000e+00 ... -8.09503253e-10\n",
      "   5.31058706e-11  0.00000000e+00]\n",
      " [-6.63412359e-05  0.00000000e+00  0.00000000e+00 ... -3.49765866e-09\n",
      "   1.13668515e-10  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print('Theta at which the cost is minimum: {}'.format(all_theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    Let's check what is the result of each test input\n",
    "    \"\"\"\n",
    "    X = np.insert(X, 0 ,values = np.ones(X.shape[0]),axis = 1)\n",
    "    X = np.matrix(X)\n",
    "    theta = np.matrix(theta)\n",
    "    \n",
    "    hyp = sigmoid(X * theta.T)\n",
    "    # we need to return the location at which hyp is maximum!!\n",
    "\n",
    "    return (np.argmax(hyp, axis = 1) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ...,\n",
       "        [ 9],\n",
       "        [ 7],\n",
       "        [10]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(data_dict['X'], all_theta)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Vs Rest Classifier Accuracy = 74.6%\n"
     ]
    }
   ],
   "source": [
    "correct = []\n",
    "for (a,b) in zip(y_pred, data_dict['y']):\n",
    "    if a == b:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print ('One Vs Rest Classifier Accuracy = {}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "In the previous part, a multi-class logistic regression was implemented to recognize handwritten digits. However, logistic regression cannot form more complex hypotheses as it is only a linear classifier. More features can be added (such as polynomial features) to logistic regression, but that can be very expensive to train.\n",
    "\n",
    "In this part, a neural network will be implemented to recognize handwritten digits using the same training set as before. The neural network will be able to represent complex models that form non-linear hypotheses. This time, there will be used parameters from a neural network that have been already trained. The goal is to implement the feedforward propagation algorithm to use the weights for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Training Set\n",
    "X = data_dict.get('X')\n",
    "y = data_dict.get('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Network\n",
    "\n",
    "The network choosen for predicting handwritten digits has 1 input layer, 1 hidden layer and 1 output layer.\n",
    "We are already provided with the weights (theta) used in training this model. Load the file and use it for prediction and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = 400  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25   # 25 hidden units\n",
    "num_labels = 10          # 10 labels, from 0 to 9  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_dict = sio.loadmat('Data/ex3weights.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1 = np.array(theta_dict.get('Theta1'))\n",
    "theta2 = np.array(theta_dict.get('Theta2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using Feed Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta1, theta2, X):\n",
    "    \"\"\"\n",
    "    This function calculates the hidden layer activation values from weights\n",
    "    and features.\n",
    "\n",
    "    a1 = feature values (X) with bias of 1\n",
    "    a2 = activation unit in hidden layer\n",
    "    a3 = hypothesis from the output layer\n",
    "    \"\"\"\n",
    "\n",
    "    [m, n] = X.shape # m = size of training set\n",
    "                     # n = number of features\n",
    "    \n",
    "    a1 = np.array(np.column_stack(((np.ones((m,1))), X))) # add bias + 1\n",
    "    z1 = np.dot(a1, np.transpose(theta1))\n",
    "    a2 = sigmoid(z1) \n",
    "\n",
    "    a2 = np.array(np.column_stack(((np.ones((m,1))), a2))) # add bias + 1\n",
    "    z2 = np.dot(a2, np.transpose(theta2))\n",
    "    a3 = sigmoid(z2)\n",
    "\n",
    "    return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting values\n",
    "\n",
    "predicted = predict(theta1, theta2, X)\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.12661530e-04, 1.74127856e-03, 2.52696959e-03, 1.84032321e-05,\n",
       "         9.36263860e-03, 3.99270267e-03, 5.51517524e-03, 4.01468105e-04,\n",
       "         6.48072305e-03, 9.95734012e-01],\n",
       "        [4.79026796e-04, 2.41495958e-03, 3.44755685e-03, 4.05616281e-05,\n",
       "         6.53412433e-03, 1.75930169e-03, 1.15788527e-02, 2.39107046e-03,\n",
       "         1.97025086e-03, 9.95696931e-01],\n",
       "        [8.85702310e-05, 3.24266731e-03, 2.55419797e-02, 2.13621788e-05,\n",
       "         3.96912754e-03, 1.02881088e-02, 3.86839058e-04, 6.22892325e-02,\n",
       "         5.49803551e-03, 9.28008397e-01]]),\n",
       " array([10, 10, 10]),\n",
       " array([[10],\n",
       "        [10],\n",
       "        [10]], dtype=uint8))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.array(np.argmax(predicted, axis=1) + 1) # python is zero-indexed\n",
    "\n",
    "predicted[:3], y_pred[:3], y[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy of Neural Network is 97.52 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluation \n",
    "\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print('Training Accuracy of Neural Network is {} %'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
